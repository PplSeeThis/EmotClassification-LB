import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
# Для лемматизации можно использовать pymorphy2, если он установлен и поддерживает украинский язык
# import pymorphy2
from sklearn.model_selection import train_test_split
import pandas as pd # Предполагается, что данные могут быть в DataFrame

# Загрузка необходимых ресурсов NLTK (однократно)
try:
    stopwords.words('russian') # Используем russian как наиболее близкий доступный в nltk, либо нужен свой список
    # Для украинского языка может потребоваться свой список стоп-слов.
    # Пример украинских стоп-слов (можно расширить):
    ukrainian_stopwords = [
        'і', 'в', 'на', 'до', 'з', 'за', 'під', 'над', 'у', 'про', 'як', 'що', 'та', 'але', 'або', 'чи',
        'був', 'була', 'було', 'були', 'є', 'буде', 'будуть', 'цей', 'ця', 'це', 'ці', 'той', 'та', 'те', 'ті',
        'мій', 'моя', 'моє', 'мої', 'твій', 'твоя', 'твоє', 'твої', 'свій', 'своя', 'своє', 'свої',
        'він', 'вона', 'воно', 'вони', 'ми', 'ви', 'ти', 'я', 'собі', 'себе', 'мені', 'тобі', 'йому', 'їй',
        'нам', 'вам', 'їм', 'мене', 'тебе', 'його', 'її', 'нас', 'вас', 'їх', 'більше', 'менше', 'все', 'всі',
        'всього', 'ніж', 'ще', 'тут', 'там', 'коли', 'де', 'куди', 'звідки', 'тому', 'через', 'ось', 'від',
        'для', 'ж', 'же', 'б', 'би', 'от', 'отак', 'так', 'такий', 'така', 'таке', 'такі', 'сам', 'сама', 'само', 'самі'
    ]
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')
    # nltk.download('wordnet') # Для лемматизации на английском, для украинского нужен другой подход
    # nltk.download('omw-1.4') # Для лемматизации на английском

# Инициализация лемматизатора (пример с pymorphy2)
# morph = pymorphy2.MorphAnalyzer(lang='uk')

def нормалізація_тексту(текст: str) -> str:
    """
    Нормализует текст: приводит к нижнему регистру, удаляет URL, HTML-теги,
    спецсимволы и лишние пробелы. Заменяет эмотиконы (базовая реализация).
    """
    # Приведение к нижнему регистру
    текст = текст.lower()

    # Замещение базовых эмотиконов (можно расширить)
    # Этот этап опционален и зависит от того, как вы хотите обрабатывать эмотиконы
    # В курсовой работе упоминается "конвертація емотиконів і смайлів у відповідні текстові мітки"
    # Здесь для примера просто удалим или заменим на слово, но лучше использовать специальные библиотеки
    # или более сложные regex для конвертации в метки типа <smile>, <sadface>
    текст = re.sub(r':\)|:\-\)|=\)|:D', ' <smile> ', текст)
    текст = re.sub(r':\(|:\-\(|=\(', ' <sadface> ', текст)

    # Удаление URL
    текст = re.sub(r'http\S+|www\S+|https\S+', '', текст, flags=re.MULTILINE)

    # Удаление HTML-тегов
    текст = re.sub(r'<.*?>', '', текст)

    # Удаление спецсимволов (оставляем буквы, цифры и пробелы)
    # Для украинского языка важно учесть украинские буквы
    текст = re.sub(r'[^а-яіїєґa-z0-9\s]', '', текст) # Добавлены украинские буквы їієґ

    # Удаление лишних пробелов
    текст = re.sub(r'\s+', ' ', текст).strip()
    return текст

def токенізація(текст: str) -> list[str]:
    """
    Разбивает текст на отдельные токены (слова и знаки пунктуации).
    """
    # NLTK word_tokenize хорошо работает для многих языков, включая те, что используют кириллицу.
    # Для украинского языка можно также рассмотреть `razdel`
    # from razdel import tokenize
    # tokens = [token.text for token in tokenize(текст)]
    return word_tokenize(текст)

def видалення_стоп_слів(токени: list[str], custom_stopwords: list[str] = None) -> list[str]:
    """
    Удаляет стоп-слова из списка токенов.
    Использует кастомный список украинских стоп-слов, если предоставлен,
    иначе пытается использовать 'russian' из NLTK или предоставленный ранее список ukrainian_stopwords.
    """
    if custom_stopwords:
        stop_words_set = set(custom_stopwords)
    else:
        try:
            # Пытаемся использовать ранее определенный список украинских стоп-слов
            stop_words_set = set(ukrainian_stopwords)
        except NameError:
            # Если ukrainian_stopwords не определен, используем русский из nltk как запасной вариант
            # Это не идеально, лучше всегда предоставлять актуальный список для украинского языка.
            print("Предупреждение: используется русский список стоп-слов из NLTK. Рекомендуется предоставить украинский.")
            stop_words_set = set(stopwords.words('russian'))

    filtered_tokens = [word for word in токени if word.casefold() not in stop_words_set and len(word) > 1] # Убираем также однобуквенные токены
    return filtered_tokens

def лематизація(токени: list[str]) -> list[str]:
    """
    Приводит слова к их начальной форме (лемме).
    Требует морфологический анализатор для украинского языка, например, pymorphy2.
    Если pymorphy2 не используется, функция вернет исходные токены с предупреждением.
    """
    # Пример с pymorphy2:
    # if 'morph' in globals():
    #     lemmatized_tokens = [morph.parse(word)[0].normal_form for word in токени]
    #     return lemmatized_tokens
    # else:
    #     print("Предупреждение: Pymorphy2 не инициализирован. Лемматизация не выполнена.")
    #     return токени

    # Базовая реализация (заглушка), так как pymorphy2 может быть не установлен.
    # В реальном проекте здесь должна быть интеграция с подходящим лемматизатором.
    print("Предупреждение: Лемматизация не реализована в этой базовой версии. Возвращаются исходные токены.")
    print("Для украинского языка рассмотрите использование библиотеки pymorphy2 или аналогичной.")
    return токени


def препроцесинг_тексту(текст: str, stopwords_list: list[str] = None) -> list[str]:
    """
    Полный конвейер предварительной обработки текста.
    """
    текст_нормалізований = нормалізація_тексту(текст)
    токени = токенізація(текст_нормалізований)
    токени_без_стоп_слів = видалення_стоп_слів(токени, custom_stopwords=stopwords_list if stopwords_list else ukrainian_stopwords)
    # лематизовані_токени = лематизація(токени_без_стоп_слів) # Раскомментировать, если лемматизация реализована
    # return лематизовані_токени
    return токени_без_стоп_слів # Возвращаем токены без стоп-слов, если лемматизация не активна

def розділення_даних(df: pd.DataFrame, text_column: str, label_column: str, test_size=0.15, val_size=0.15, random_state=42):
    """
    Разделяет DataFrame на обучающую, валидационную и тестовую выборки.
    Val_size указывается как доля от исходного датасета, а не от оставшегося после train_test_split.
    """
    if not (0 < test_size < 1 and 0 < val_size < 1 and (test_size + val_size) < 1):
        raise ValueError("test_size и val_size должны быть между 0 и 1, и их сумма должна быть меньше 1.")

    # Сначала отделяем тестовую выборку
    train_val_df, test_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=df[label_column] if label_column in df.columns and df[label_column].nunique() > 1 else None
    )

    # Затем отделяем валидационную выборку из оставшейся части (train_val_df)
    # Рассчитываем долю для валидационной выборки относительно train_val_df
    # val_size_relative = val_size / (1 - test_size)
    # Если мы хотим, чтобы val_size был долей от *исходного* датасета, то:
    # (1 - test_size) * X = val_size  => X = val_size / (1-test_size)
    
    # В курсовой указано: 70% train, 15% validation, 15% test
    # train_size = 1.0 - test_size - val_size
    # Это значит, что после отделения test_set (15%), остается 85%.
    # Из этих 85% нужно взять val_set, чтобы он составил 15% от *оригинала*.
    # То есть, val_set должен быть 15/85 от оставшихся данных.
    
    relative_val_size = val_size / (1.0 - test_size)


    train_df, val_df = train_test_split(
        train_val_df,
        test_size=relative_val_size, # Доля для валидации от оставшихся данных
        random_state=random_state,
        stratify=train_val_df[label_column] if label_column in train_val_df.columns and train_val_df[label_column].nunique() > 1 else None
    )
    
    print(f"Размер исходного датасета: {len(df)}")
    print(f"Размер обучающей выборки: {len(train_df)} ({len(train_df)/len(df):.2%})")
    print(f"Размер валидационной выборки: {len(val_df)} ({len(val_df)/len(df):.2%})")
    print(f"Размер тестовой выборки: {len(test_df)} ({len(test_df)/len(df):.2%})")

    return train_df, val_df, test_df


if __name__ == '__main__':
    # Пример использования:
    приклад_тексту = "Привіт, Світ! :) Це тестовий рядок з URL: https://example.com, HTML: <p>текст</p> і деякими #спецсимволами! Як твої справи?"
    print(f"Оригінальний текст: {приклад_тексту}")

    текст_нормалізований = нормалізація_тексту(приклад_тексту)
    print(f"Нормалізований текст: {текст_нормалізований}")

    токени = токенізація(текст_нормалізований)
    print(f"Токени: {токени}")

    токени_без_стоп_слів = видалення_стоп_слів(токени, ukrainian_stopwords)
    print(f"Токени без стоп-слів: {токени_без_стоп_слів}")

    # Для лемматизации:
    # лематизовані_токени = лематизація(токени_без_стоп_слів)
    # print(f"Лематизовані токени: {лематизовані_токени}")

    оброблені_токени = препроцесинг_тексту(приклад_тексту, ukrainian_stopwords)
    print(f"Результат препроцесингу (список токенів): {оброблені_токени}")
    print(f"Результат препроцесингу (рядок): {' '.join(оброблені_токени)}")

    # Пример разделения данных
    # Создадим пример DataFrame
    data = {
        'text': [
            "Це позитивний відгук, все чудово!", "Дуже сподобалось, рекомендую.", "Жахливий сервіс, нікому не раджу.",
            "Нейтральний коментар про погоду.", "Я в захваті від цієї книги!", "Розчарований якістю товару.",
            "Фільм просто супер, емоції переповнюють.", "Звичайний день, нічого особливого.", "Це було страшно, але захоплююче.",
            "Відчуваю сум через цю новину.", "Я злий на таку несправедливість.", "Дивно, але факт."
        ] * 10, # Увеличим датасет для примера стратификации
        'emotion': [
            "радість", "радість", "гнів", "нейтральний", "радість", "смуток",
            "радість", "нейтральний", "страх", "смуток", "гнів", "здивування"
        ] * 10
    }
    sample_df = pd.DataFrame(data)

    print(f"\nПриклад датасету (перші 5 рядків):\n{sample_df.head()}")
    print(f"Розподіл емоцій в датасеті:\n{sample_df['emotion'].value_counts(normalize=True)}")


    try:
        train_data, val_data, test_data = розділення_даних(sample_df, 'text', 'emotion', test_size=0.15, val_size=0.15)
        # print(f"\nОбучающая выборка (первые 2):\n{train_data.head(2)}")
        # print(f"Валидационная выборка (первые 2):\n{val_data.head(2)}")
        # print(f"Тестовая выборка (первые 2):\n{test_data.head(2)}")
    except ValueError as e:
        print(f"Ошибка при разделении данных: {e}")

